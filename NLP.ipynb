{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ninja-draw-coder/PythonProjects/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-U9mB5mV5Qo",
        "outputId": "fe12601d-f687-4f07-dc28-78c625f08930"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "aRYvCwU7WEIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RW-wX3etWSyc",
        "outputId": "17c3924b-b134-4dba-c297-d418e2f6d74b"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> punkt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "    Downloading package punkt to /root/nltk_data...\n",
            "      Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> l\n",
            "\n",
            "Packages:\n",
            "  [ ] abc................. Australian Broadcasting Commission 2006\n",
            "  [ ] alpino.............. Alpino Dutch Treebank\n",
            "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] averaged_perceptron_tagger_eng Averaged Perceptron Tagger (JSON)\n",
            "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [ ] averaged_perceptron_tagger_rus Averaged Perceptron Tagger (Russian)\n",
            "  [ ] basque_grammars..... Grammars for Basque\n",
            "  [ ] bcp47............... BCP-47 Language Tags\n",
            "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [ ] book_grammars....... Grammars from NLTK Book\n",
            "  [ ] brown............... Brown Corpus\n",
            "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [ ] cess_cat............ CESS-CAT Treebank\n",
            "  [ ] cess_esp............ CESS-ESP Treebank\n",
            "  [ ] chat80.............. Chat-80 Data Files\n",
            "  [ ] city_database....... City Database\n",
            "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [ ] comparative_sentences Comparative Sentence Dataset\n",
            "Hit Enter to continue: u\n",
            "  [ ] comtrans............ ComTrans Corpus Sample\n",
            "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
            "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
            "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
            "                           and Basque Subset)\n",
            "  [ ] crubadan............ Crubadan Corpus\n",
            "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
            "  [ ] dolch............... Dolch Word List\n",
            "  [ ] english_wordnet..... Open English Wordnet\n",
            "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
            "                           Corpus\n",
            "  [ ] extended_omw........ Extended Open Multilingual WordNet\n",
            "  [ ] floresta............ Portuguese Treebank\n",
            "  [ ] framenet_v15........ FrameNet 1.5\n",
            "  [ ] framenet_v17........ FrameNet 1.7\n",
            "  [ ] gazetteers.......... Gazeteer Lists\n",
            "  [ ] genesis............. Genesis Corpus\n",
            "  [ ] gutenberg........... Project Gutenberg Selections\n",
            "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
            "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
            "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
            "Hit Enter to continue: q\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZYrkhr9ohFb",
        "outputId": "c7935d07-0b81-4245-852e-addb93807fc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "#Importing necessary NLTK modules for preprocessing tasks\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize                # For tokenizing sentences and words\n",
        "from nltk.corpus import stopwords, wordnet                            # For stopword removal and lemmatization support\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer                # For stemming and lemmatization\n",
        "from nltk import pos_tag, ne_chunk                                    # For Part-of-Speech tagging and Named Entity Recognition\n",
        "from nltk.chunk import tree2conlltags                                 # To convert chunk trees into IOB-tagged format\n",
        "from nltk.util import ngrams                                          # For creating n-grams (unigrams, bigrams, trigrams)\n",
        "from nltk.wsd import lesk                                             # For Word Sense Disambiguation using Lesk algorithm\n",
        "\n",
        "#Downloading required NLTK datasets and models\n",
        "\n",
        "nltk.download('punkt')                            # Tokenizer models for word and sentence tokenization\n",
        "nltk.download('punkt_tab')                        # Internal table used by 'punkt' tokenizer (optional in most cases)\n",
        "nltk.download('averaged_perceptron_tagger')       # POS tagging model\n",
        "nltk.download(\"averaged_perceptron_tagger_eng\")   # English-specific POS tagger data (optional/legacy)\n",
        "nltk.download('stopwords')                        # List of common English stopwords\n",
        "nltk.download('maxent_ne_chunker')                # Named Entity Recognition chunker model\n",
        "nltk.download(\"maxent_ne_chunker_tab\")            # Chunker support file (optional in most cases)\n",
        "nltk.download('words')                            # Word list used by the NER chunker\n",
        "nltk.download('wordnet')                          # WordNet lexical database for lemmatization and WSD\n",
        "nltk.download('omw-1.4')                          # Open Multilingual WordNet for lemmatizer language support\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = \"\"\"Apple Inc. is looking at buying U.K. startup for $1 billion.\n",
        "The CEO Tim Cook made the announcement during the annual developer conference in California.\n",
        "He said the move will boost Apple’s AI capabilities significantly.\"\"\""
      ],
      "metadata": {
        "id": "2wb4BLDOolrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1. Sentence and Word Tokenization**\n",
        "**Definition:**  \n",
        "Tokenization is the process of splitting text into smaller units.  \n",
        "- **Sentence Tokenization** splits a paragraph into sentences.  \n",
        "- **Word Tokenization** splits a sentence into words."
      ],
      "metadata": {
        "id": "nCutZ23MpY8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence Tokenization\n",
        "sentences = sent_tokenize(paragraph)\n",
        "print(\"Sentence Tokenization:\\n\", sentences)\n",
        "\n",
        "# Word Tokenization\n",
        "words = word_tokenize(paragraph)\n",
        "print(\"\\nWord Tokenization:\\n\", words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0HwmCXbowQK",
        "outputId": "38d2fc1a-5460-48ab-c866-0210146533c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Tokenization:\n",
            " ['Apple Inc. is looking at buying U.K. startup for $1 billion.', 'The CEO Tim Cook made the announcement during the annual developer conference in California.', 'He said the move will boost Apple’s AI capabilities significantly.']\n",
            "\n",
            "Word Tokenization:\n",
            " ['Apple', 'Inc.', 'is', 'looking', 'at', 'buying', 'U.K.', 'startup', 'for', '$', '1', 'billion', '.', 'The', 'CEO', 'Tim', 'Cook', 'made', 'the', 'announcement', 'during', 'the', 'annual', 'developer', 'conference', 'in', 'California', '.', 'He', 'said', 'the', 'move', 'will', 'boost', 'Apple', '’', 's', 'AI', 'capabilities', 'significantly', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**2. POS Tagging**\n",
        "**Definition:**  \n",
        "It assigns a part of speech to each word (e.g., noun, verb, adjective)."
      ],
      "metadata": {
        "id": "LFQGQWNkpcMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tags = pos_tag(words)\n",
        "print(\"\\nPOS Tagging:\\n\", pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOj3AZGOo6FB",
        "outputId": "b5306141-b7e1-46c8-8b3d-a8e2ccc35b92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "POS Tagging:\n",
            " [('Apple', 'NNP'), ('Inc.', 'NNP'), ('is', 'VBZ'), ('looking', 'VBG'), ('at', 'IN'), ('buying', 'VBG'), ('U.K.', 'NNP'), ('startup', 'NN'), ('for', 'IN'), ('$', '$'), ('1', 'CD'), ('billion', 'CD'), ('.', '.'), ('The', 'DT'), ('CEO', 'NNP'), ('Tim', 'NNP'), ('Cook', 'NNP'), ('made', 'VBD'), ('the', 'DT'), ('announcement', 'NN'), ('during', 'IN'), ('the', 'DT'), ('annual', 'JJ'), ('developer', 'NN'), ('conference', 'NN'), ('in', 'IN'), ('California', 'NNP'), ('.', '.'), ('He', 'PRP'), ('said', 'VBD'), ('the', 'DT'), ('move', 'NN'), ('will', 'MD'), ('boost', 'VB'), ('Apple', 'NNP'), ('’', 'NNP'), ('s', 'VBD'), ('AI', 'NNP'), ('capabilities', 'NNS'), ('significantly', 'RB'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**3. Stopwords Removal**\n",
        "**Definition:**  \n",
        "Stopwords are common words (like \"the\", \"is\", \"and\") that add little meaning and are often removed during preprocessing."
      ],
      "metadata": {
        "id": "3SYnwgnIpdjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words and word.isalpha()]\n",
        "print(\"\\nAfter Stopword Removal:\\n\", filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5lzyyADpDi-",
        "outputId": "fdca6be6-4e85-4ad2-9021-8e965325a81b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After Stopword Removal:\n",
            " ['Apple', 'looking', 'buying', 'startup', 'billion', 'CEO', 'Tim', 'Cook', 'made', 'announcement', 'annual', 'developer', 'conference', 'California', 'said', 'move', 'boost', 'Apple', 'AI', 'capabilities', 'significantly']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**4. Stemming**\n",
        "**Definition:**  \n",
        "Stemming reduces words to their base or root form (e.g., \"running\" → \"run\").  \n",
        "It may not always produce real words.\n"
      ],
      "metadata": {
        "id": "c1y26ZAVpexG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
        "print(\"\\nStemming:\\n\", stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GL5P2nJdpFB8",
        "outputId": "69e8c24f-134a-42c8-f0c2-77632cbb4baf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stemming:\n",
            " ['appl', 'look', 'buy', 'startup', 'billion', 'ceo', 'tim', 'cook', 'made', 'announc', 'annual', 'develop', 'confer', 'california', 'said', 'move', 'boost', 'appl', 'ai', 'capabl', 'significantli']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**5. Lemmatization**\n",
        "**Definition:**  \n",
        "Lemmatization also reduces words to their base form, but ensures the result is a real word (e.g., \"was\" → \"be\", \"better\" → \"good\")."
      ],
      "metadata": {
        "id": "jL2jzVWJpfxW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "print(\"\\nLemmatization:\\n\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNmjTwF8pH-C",
        "outputId": "ba22e195-79fa-45a4-9953-8909a5e16531"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lemmatization:\n",
            " ['Apple', 'looking', 'buying', 'startup', 'billion', 'CEO', 'Tim', 'Cook', 'made', 'announcement', 'annual', 'developer', 'conference', 'California', 'said', 'move', 'boost', 'Apple', 'AI', 'capability', 'significantly']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**6. Named Entity Recognition (NER)**\n",
        "**Definition:**  \n",
        "NER identifies named entities in text such as names of people, organizations, locations, etc."
      ],
      "metadata": {
        "id": "0M5untpZpgmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ner_tree = ne_chunk(pos_tags)\n",
        "named_entities = tree2conlltags(ner_tree)\n",
        "print(\"\\nNamed Entity Recognition:\\n\", named_entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmPlrG-kpJbL",
        "outputId": "b983984d-c6a4-4f64-b373-cbbabd9b39f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Named Entity Recognition:\n",
            " [('Apple', 'NNP', 'B-PERSON'), ('Inc.', 'NNP', 'B-ORGANIZATION'), ('is', 'VBZ', 'O'), ('looking', 'VBG', 'O'), ('at', 'IN', 'O'), ('buying', 'VBG', 'O'), ('U.K.', 'NNP', 'O'), ('startup', 'NN', 'O'), ('for', 'IN', 'O'), ('$', '$', 'O'), ('1', 'CD', 'O'), ('billion', 'CD', 'O'), ('.', '.', 'O'), ('The', 'DT', 'O'), ('CEO', 'NNP', 'B-ORGANIZATION'), ('Tim', 'NNP', 'I-ORGANIZATION'), ('Cook', 'NNP', 'I-ORGANIZATION'), ('made', 'VBD', 'O'), ('the', 'DT', 'O'), ('announcement', 'NN', 'O'), ('during', 'IN', 'O'), ('the', 'DT', 'O'), ('annual', 'JJ', 'O'), ('developer', 'NN', 'O'), ('conference', 'NN', 'O'), ('in', 'IN', 'O'), ('California', 'NNP', 'B-GPE'), ('.', '.', 'O'), ('He', 'PRP', 'O'), ('said', 'VBD', 'O'), ('the', 'DT', 'O'), ('move', 'NN', 'O'), ('will', 'MD', 'O'), ('boost', 'VB', 'O'), ('Apple', 'NNP', 'B-PERSON'), ('’', 'NNP', 'O'), ('s', 'VBD', 'O'), ('AI', 'NNP', 'O'), ('capabilities', 'NNS', 'O'), ('significantly', 'RB', 'O'), ('.', '.', 'O')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**7. Word Sense Disambiguation (WSD)**\n",
        "**Definition:**  \n",
        "WSD determines the correct meaning (sense) of a word based on context.  \n",
        "We use the Lesk algorithm here.\n",
        "\n",
        "**Example:**\n",
        "Word = \"move\"\n",
        "Sentence = \"He said the move will boost Apple’s AI capabilities significantly.\""
      ],
      "metadata": {
        "id": "J0J3M7NkphT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the sentence containing the word \"move\"\n",
        "context_sentence = \"He said the move will boost Apple’s AI capabilities significantly.\"\n",
        "context_words = word_tokenize(context_sentence)\n",
        "wsd_result = lesk(context_words, 'move')\n",
        "print(\"\\nWSD for 'move':\\n\", wsd_result)\n",
        "print(\"Definition:\", wsd_result.definition())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MygsK5XnpTWi",
        "outputId": "ba6931a3-8994-4715-c6fd-d2a8993b5966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WSD for 'move':\n",
            " Synset('move.v.15')\n",
            "Definition: have a turn; make one's move in a game\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**N-Grams (Unigrams, Bigrams, Trigrams)**\n",
        "**Definition:**  \n",
        "N-grams are continuous sequences of 'n' items from a given text.\n",
        "- **Unigram** = Single words  \n",
        "- **Bigram** = Pairs of words  \n",
        "- **Trigram** = Triplets of words\n"
      ],
      "metadata": {
        "id": "1JN830D0piXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use only alphabetic filtered words\n",
        "print(\"\\nUnigrams:\")\n",
        "print(list(ngrams(filtered_words, 1)))\n",
        "\n",
        "print(\"\\nBigrams:\")\n",
        "print(list(ngrams(filtered_words, 2)))\n",
        "\n",
        "print(\"\\nTrigrams:\")\n",
        "print(list(ngrams(filtered_words, 3)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JnPjvRMomVh",
        "outputId": "1d2e08a0-e2ac-48ed-f6e4-21dab1f45522"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Unigrams:\n",
            "[('Apple',), ('looking',), ('buying',), ('startup',), ('billion',), ('CEO',), ('Tim',), ('Cook',), ('made',), ('announcement',), ('annual',), ('developer',), ('conference',), ('California',), ('said',), ('move',), ('boost',), ('Apple',), ('AI',), ('capabilities',), ('significantly',)]\n",
            "\n",
            "Bigrams:\n",
            "[('Apple', 'looking'), ('looking', 'buying'), ('buying', 'startup'), ('startup', 'billion'), ('billion', 'CEO'), ('CEO', 'Tim'), ('Tim', 'Cook'), ('Cook', 'made'), ('made', 'announcement'), ('announcement', 'annual'), ('annual', 'developer'), ('developer', 'conference'), ('conference', 'California'), ('California', 'said'), ('said', 'move'), ('move', 'boost'), ('boost', 'Apple'), ('Apple', 'AI'), ('AI', 'capabilities'), ('capabilities', 'significantly')]\n",
            "\n",
            "Trigrams:\n",
            "[('Apple', 'looking', 'buying'), ('looking', 'buying', 'startup'), ('buying', 'startup', 'billion'), ('startup', 'billion', 'CEO'), ('billion', 'CEO', 'Tim'), ('CEO', 'Tim', 'Cook'), ('Tim', 'Cook', 'made'), ('Cook', 'made', 'announcement'), ('made', 'announcement', 'annual'), ('announcement', 'annual', 'developer'), ('annual', 'developer', 'conference'), ('developer', 'conference', 'California'), ('conference', 'California', 'said'), ('California', 'said', 'move'), ('said', 'move', 'boost'), ('move', 'boost', 'Apple'), ('boost', 'Apple', 'AI'), ('Apple', 'AI', 'capabilities'), ('AI', 'capabilities', 'significantly')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qgKVu4rSoroj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}